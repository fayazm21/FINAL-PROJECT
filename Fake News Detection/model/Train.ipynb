{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras import *\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df= pd.read_csv(\"trains.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sms_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xt/shrx8k3j6hn4zm7_g3_l2c3r0000gn/T/ipykernel_2235/2674229937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msms_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sms_df' is not defined"
     ]
    }
   ],
   "source": [
    "sms_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df=sms_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-5cccea66aba9>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sms_df['label']= pd.to_numeric(sms_df['label'])\n"
     ]
    }
   ],
   "source": [
    "sms_df['label']= pd.to_numeric(sms_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sms_df['text']\n",
    "labels = sms_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_texts, test_texts, other_labels, test_labels  = train_test_split(texts, labels, test_size=0.1, random_state=302)\n",
    "#Create validation sample\n",
    "train_texts, valid_texts, train_labels, valid_labels  = train_test_split(other_texts, other_labels, test_size=0.2, random_state=302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Convert words into word ids\n",
    "meanLength = np.mean([len(item.split(\" \")) for item in train_texts])\n",
    "MAX_SENTENCE_LENGTH = int(meanLength + 5) # we let a text go 10 words longer than the mean text length.\n",
    "\n",
    "# Convert train, validation, and test text into lists with word ids\n",
    "trainFeatures = tokenizer.texts_to_sequences(train_texts)\n",
    "trainFeatures = pad_sequences(trainFeatures, MAX_SENTENCE_LENGTH, padding='post')\n",
    "trainLabels = train_labels.values\n",
    "\n",
    "validFeatures = tokenizer.texts_to_sequences(valid_texts)\n",
    "validFeatures = pad_sequences(validFeatures, MAX_SENTENCE_LENGTH, padding='post')\n",
    "validLabels = valid_labels.values\n",
    "\n",
    "testFeatures = tokenizer.texts_to_sequences(test_texts)\n",
    "testFeatures = pad_sequences(testFeatures, MAX_SENTENCE_LENGTH, padding='post')\n",
    "testLabels = test_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "547"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS_SIZE = 16\n",
    "KERNEL_SIZE = 5\n",
    "\n",
    "# Define embeddings dimensions (columns in matrix fed into CNN and nodes in hidden layer of built-in keras function)\n",
    "EMBEDDINGS_DIM = 11\n",
    "\n",
    "# Hyperparameters for model tuning\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 547, 11)           275011    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 543, 16)           896       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 543, 16)           0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 276,052\n",
      "Trainable params: 276,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Word CNN\n",
    "model = Sequential()\n",
    "\n",
    "# We use built-in keras funtion to generate embeddings. Another option is pre-trained embeddings with Word2vec or GloVe.\n",
    "model.add(Embedding(input_dim=VOCABULARY_SIZE + 1, output_dim=EMBEDDINGS_DIM, input_length=len(trainFeatures[0])))\n",
    "model.add(Conv1D(FILTERS_SIZE, KERNEL_SIZE, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "optimizer = optimizers.Adam(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "224/224 [==============================] - 4s 16ms/step - loss: 0.6504 - accuracy: 0.6120 - val_loss: 0.5433 - val_accuracy: 0.8553\n",
      "Epoch 2/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.4224 - accuracy: 0.8284 - val_loss: 0.3189 - val_accuracy: 0.8821\n",
      "Epoch 3/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.2979 - accuracy: 0.8893 - val_loss: 0.2604 - val_accuracy: 0.9106\n",
      "Epoch 4/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.2161 - accuracy: 0.9275 - val_loss: 0.2188 - val_accuracy: 0.9201\n",
      "Epoch 5/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.1596 - accuracy: 0.9440 - val_loss: 0.2010 - val_accuracy: 0.9168\n",
      "Epoch 6/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.1158 - accuracy: 0.9626 - val_loss: 0.1604 - val_accuracy: 0.9358\n",
      "Epoch 7/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0897 - accuracy: 0.9744 - val_loss: 0.1611 - val_accuracy: 0.9369\n",
      "Epoch 8/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0627 - accuracy: 0.9835 - val_loss: 0.1527 - val_accuracy: 0.9341\n",
      "Epoch 9/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0497 - accuracy: 0.9871 - val_loss: 0.1451 - val_accuracy: 0.9397\n",
      "Epoch 10/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0391 - accuracy: 0.9892 - val_loss: 0.1487 - val_accuracy: 0.9385\n",
      "Epoch 11/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0349 - accuracy: 0.9897 - val_loss: 0.1526 - val_accuracy: 0.9363\n",
      "Epoch 12/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0272 - accuracy: 0.9927 - val_loss: 0.1592 - val_accuracy: 0.9380\n",
      "Epoch 13/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0278 - accuracy: 0.9925 - val_loss: 0.1695 - val_accuracy: 0.9285\n",
      "Epoch 14/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0246 - accuracy: 0.9932 - val_loss: 0.1637 - val_accuracy: 0.9358\n",
      "Epoch 15/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0224 - accuracy: 0.9932 - val_loss: 0.1817 - val_accuracy: 0.9291\n",
      "Epoch 16/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0242 - accuracy: 0.9927 - val_loss: 0.1715 - val_accuracy: 0.9369\n",
      "Epoch 17/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0222 - accuracy: 0.9926 - val_loss: 0.1708 - val_accuracy: 0.9391\n",
      "Epoch 18/18\n",
      "224/224 [==============================] - 3s 15ms/step - loss: 0.0226 - accuracy: 0.9941 - val_loss: 0.1768 - val_accuracy: 0.9391\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(trainFeatures, trainLabels, validation_data = (validFeatures, validLabels), batch_size=BATCH_SIZE, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt = [\"WASHINGTON/JERUSALEM (Reuters) - During his 20\"]\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   148 1368  139  134   21  417]]\n",
      "1/1 - 0s\n",
      "[0.00733402]\n"
     ]
    }
   ],
   "source": [
    "twt = pad_sequences(twt, maxlen=720, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=20,verbose = 2)[0]\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "output_dir = re.sub('Model and data', 'Flask application', current_dir)\n",
    "os.chdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
